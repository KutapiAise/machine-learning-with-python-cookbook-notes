{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:machine_learning_cookbook]",
      "language": "python",
      "name": "conda-env-machine_learning_cookbook-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Chapter 6 - Handling Text-checkpoint.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLeje-RjcTJT",
        "colab_type": "text"
      },
      "source": [
        "## 6.1 Cleaning Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWuPJhYlcTJY",
        "colab_type": "code",
        "colab": {},
        "outputId": "e5918e59-3ea2-4ff0-c412-e0dced2bd826"
      },
      "source": [
        "text_data = [\"  Interrobang. By aishwarya Henriette    \",\n",
        "            \"Parking And Going. By Karl Gautier\",\n",
        "            \"    Today Is The night. By Jarek Prakash\"]\n",
        "\n",
        "# strip whitespaces\n",
        "strip_whitespace = [string.strip() for string in text_data]\n",
        "strip_whitespace"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Interrobang. By aishwarya Henriette',\n",
              " 'Parking And Going. By Karl Gautier',\n",
              " 'Today Is The night. By Jarek Prakash']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeDT7B8OcTJj",
        "colab_type": "code",
        "colab": {},
        "outputId": "d7aad9f7-efcb-459a-9502-e34891528405"
      },
      "source": [
        "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
        "remove_periods"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Interrobang By aishwarya Henriette',\n",
              " 'Parking And Going By Karl Gautier',\n",
              " 'Today Is The night By Jarek Prakash']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69GhuXkncTJr",
        "colab_type": "code",
        "colab": {},
        "outputId": "85918473-edc1-4f75-c012-73a62a1224fb"
      },
      "source": [
        "def capitalizer(string: str) -> str:\n",
        "    return string.upper()\n",
        "\n",
        "[capitalizer(string) for string in remove_periods]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
              " 'PARKING AND GOING BY KARL GAUTIER',\n",
              " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEqSxADTcTJy",
        "colab_type": "code",
        "colab": {},
        "outputId": "bc46cc16-1a6c-4f8a-c3e0-9c34b0305bf3"
      },
      "source": [
        "import re\n",
        "\n",
        "def replace_letters_with_X(string: str) -> str:\n",
        "    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
        "\n",
        "[replace_letters_with_X(string) for string in remove_periods]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
              " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
              " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7t2-Lm8cTJ6",
        "colab_type": "text"
      },
      "source": [
        "### See Also\n",
        "* Beginners Tutorial for Regular Expressions in Python (https://www.analyticsvidhya.com/blog/2015/06/regular-expression-python/)\n",
        "\n",
        "## 6.2 Parsing and Cleaning HTML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZW3MSVLcTJ8",
        "colab_type": "code",
        "colab": {},
        "outputId": "5d16474c-461c-4007-c439-030125754ba1"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = \"\"\"\n",
        "    <div class='full_name'><span style='font-weight:bold'>Yan</span> Chin</div>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html)\n",
        "\n",
        "soup.find(\"div\", {\"class\": \"full_name\"}).text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yan Chin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QzB-SSbcTKE",
        "colab_type": "text"
      },
      "source": [
        "### See Also\n",
        "* Beautiful Soup documentation (https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "\n",
        "## 6.3 Removing Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShIu3dL8cTKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "71e0f939-479e-4c5a-e648-accb903da6b1"
      },
      "source": [
        "import unicodedata\n",
        "import sys\n",
        "\n",
        "text_data = ['Hi!!! I. Love. This. Song.....', '10000% Agree!!!! #LoveIT', 'Right?!?!']\n",
        "\n",
        "# create a dictionary of punctuation characters\n",
        "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
        "\n",
        "print(punctuation)\n",
        "# for each string, remove any punctuation characters\n",
        "[string.translate(punctuation) for string in text_data]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{33: None, 34: None, 35: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 63: None, 64: None, 91: None, 92: None, 93: None, 95: None, 123: None, 125: None, 161: None, 167: None, 171: None, 182: None, 183: None, 187: None, 191: None, 894: None, 903: None, 1370: None, 1371: None, 1372: None, 1373: None, 1374: None, 1375: None, 1417: None, 1418: None, 1470: None, 1472: None, 1475: None, 1478: None, 1523: None, 1524: None, 1545: None, 1546: None, 1548: None, 1549: None, 1563: None, 1566: None, 1567: None, 1642: None, 1643: None, 1644: None, 1645: None, 1748: None, 1792: None, 1793: None, 1794: None, 1795: None, 1796: None, 1797: None, 1798: None, 1799: None, 1800: None, 1801: None, 1802: None, 1803: None, 1804: None, 1805: None, 2039: None, 2040: None, 2041: None, 2096: None, 2097: None, 2098: None, 2099: None, 2100: None, 2101: None, 2102: None, 2103: None, 2104: None, 2105: None, 2106: None, 2107: None, 2108: None, 2109: None, 2110: None, 2142: None, 2404: None, 2405: None, 2416: None, 2800: None, 3572: None, 3663: None, 3674: None, 3675: None, 3844: None, 3845: None, 3846: None, 3847: None, 3848: None, 3849: None, 3850: None, 3851: None, 3852: None, 3853: None, 3854: None, 3855: None, 3856: None, 3857: None, 3858: None, 3860: None, 3898: None, 3899: None, 3900: None, 3901: None, 3973: None, 4048: None, 4049: None, 4050: None, 4051: None, 4052: None, 4057: None, 4058: None, 4170: None, 4171: None, 4172: None, 4173: None, 4174: None, 4175: None, 4347: None, 4960: None, 4961: None, 4962: None, 4963: None, 4964: None, 4965: None, 4966: None, 4967: None, 4968: None, 5120: None, 5741: None, 5742: None, 5787: None, 5788: None, 5867: None, 5868: None, 5869: None, 5941: None, 5942: None, 6100: None, 6101: None, 6102: None, 6104: None, 6105: None, 6106: None, 6144: None, 6145: None, 6146: None, 6147: None, 6148: None, 6149: None, 6150: None, 6151: None, 6152: None, 6153: None, 6154: None, 6468: None, 6469: None, 6686: None, 6687: None, 6816: None, 6817: None, 6818: None, 6819: None, 6820: None, 6821: None, 6822: None, 6824: None, 6825: None, 6826: None, 6827: None, 6828: None, 6829: None, 7002: None, 7003: None, 7004: None, 7005: None, 7006: None, 7007: None, 7008: None, 7164: None, 7165: None, 7166: None, 7167: None, 7227: None, 7228: None, 7229: None, 7230: None, 7231: None, 7294: None, 7295: None, 7360: None, 7361: None, 7362: None, 7363: None, 7364: None, 7365: None, 7366: None, 7367: None, 7379: None, 8208: None, 8209: None, 8210: None, 8211: None, 8212: None, 8213: None, 8214: None, 8215: None, 8216: None, 8217: None, 8218: None, 8219: None, 8220: None, 8221: None, 8222: None, 8223: None, 8224: None, 8225: None, 8226: None, 8227: None, 8228: None, 8229: None, 8230: None, 8231: None, 8240: None, 8241: None, 8242: None, 8243: None, 8244: None, 8245: None, 8246: None, 8247: None, 8248: None, 8249: None, 8250: None, 8251: None, 8252: None, 8253: None, 8254: None, 8255: None, 8256: None, 8257: None, 8258: None, 8259: None, 8261: None, 8262: None, 8263: None, 8264: None, 8265: None, 8266: None, 8267: None, 8268: None, 8269: None, 8270: None, 8271: None, 8272: None, 8273: None, 8275: None, 8276: None, 8277: None, 8278: None, 8279: None, 8280: None, 8281: None, 8282: None, 8283: None, 8284: None, 8285: None, 8286: None, 8317: None, 8318: None, 8333: None, 8334: None, 8968: None, 8969: None, 8970: None, 8971: None, 9001: None, 9002: None, 10088: None, 10089: None, 10090: None, 10091: None, 10092: None, 10093: None, 10094: None, 10095: None, 10096: None, 10097: None, 10098: None, 10099: None, 10100: None, 10101: None, 10181: None, 10182: None, 10214: None, 10215: None, 10216: None, 10217: None, 10218: None, 10219: None, 10220: None, 10221: None, 10222: None, 10223: None, 10627: None, 10628: None, 10629: None, 10630: None, 10631: None, 10632: None, 10633: None, 10634: None, 10635: None, 10636: None, 10637: None, 10638: None, 10639: None, 10640: None, 10641: None, 10642: None, 10643: None, 10644: None, 10645: None, 10646: None, 10647: None, 10648: None, 10712: None, 10713: None, 10714: None, 10715: None, 10748: None, 10749: None, 11513: None, 11514: None, 11515: None, 11516: None, 11518: None, 11519: None, 11632: None, 11776: None, 11777: None, 11778: None, 11779: None, 11780: None, 11781: None, 11782: None, 11783: None, 11784: None, 11785: None, 11786: None, 11787: None, 11788: None, 11789: None, 11790: None, 11791: None, 11792: None, 11793: None, 11794: None, 11795: None, 11796: None, 11797: None, 11798: None, 11799: None, 11800: None, 11801: None, 11802: None, 11803: None, 11804: None, 11805: None, 11806: None, 11807: None, 11808: None, 11809: None, 11810: None, 11811: None, 11812: None, 11813: None, 11814: None, 11815: None, 11816: None, 11817: None, 11818: None, 11819: None, 11820: None, 11821: None, 11822: None, 11824: None, 11825: None, 11826: None, 11827: None, 11828: None, 11829: None, 11830: None, 11831: None, 11832: None, 11833: None, 11834: None, 11835: None, 11836: None, 11837: None, 11838: None, 11839: None, 11840: None, 11841: None, 11842: None, 11843: None, 11844: None, 12289: None, 12290: None, 12291: None, 12296: None, 12297: None, 12298: None, 12299: None, 12300: None, 12301: None, 12302: None, 12303: None, 12304: None, 12305: None, 12308: None, 12309: None, 12310: None, 12311: None, 12312: None, 12313: None, 12314: None, 12315: None, 12316: None, 12317: None, 12318: None, 12319: None, 12336: None, 12349: None, 12448: None, 12539: None, 42238: None, 42239: None, 42509: None, 42510: None, 42511: None, 42611: None, 42622: None, 42738: None, 42739: None, 42740: None, 42741: None, 42742: None, 42743: None, 43124: None, 43125: None, 43126: None, 43127: None, 43214: None, 43215: None, 43256: None, 43257: None, 43258: None, 43260: None, 43310: None, 43311: None, 43359: None, 43457: None, 43458: None, 43459: None, 43460: None, 43461: None, 43462: None, 43463: None, 43464: None, 43465: None, 43466: None, 43467: None, 43468: None, 43469: None, 43486: None, 43487: None, 43612: None, 43613: None, 43614: None, 43615: None, 43742: None, 43743: None, 43760: None, 43761: None, 44011: None, 64830: None, 64831: None, 65040: None, 65041: None, 65042: None, 65043: None, 65044: None, 65045: None, 65046: None, 65047: None, 65048: None, 65049: None, 65072: None, 65073: None, 65074: None, 65075: None, 65076: None, 65077: None, 65078: None, 65079: None, 65080: None, 65081: None, 65082: None, 65083: None, 65084: None, 65085: None, 65086: None, 65087: None, 65088: None, 65089: None, 65090: None, 65091: None, 65092: None, 65093: None, 65094: None, 65095: None, 65096: None, 65097: None, 65098: None, 65099: None, 65100: None, 65101: None, 65102: None, 65103: None, 65104: None, 65105: None, 65106: None, 65108: None, 65109: None, 65110: None, 65111: None, 65112: None, 65113: None, 65114: None, 65115: None, 65116: None, 65117: None, 65118: None, 65119: None, 65120: None, 65121: None, 65123: None, 65128: None, 65130: None, 65131: None, 65281: None, 65282: None, 65283: None, 65285: None, 65286: None, 65287: None, 65288: None, 65289: None, 65290: None, 65292: None, 65293: None, 65294: None, 65295: None, 65306: None, 65307: None, 65311: None, 65312: None, 65339: None, 65340: None, 65341: None, 65343: None, 65371: None, 65373: None, 65375: None, 65376: None, 65377: None, 65378: None, 65379: None, 65380: None, 65381: None, 65792: None, 65793: None, 65794: None, 66463: None, 66512: None, 66927: None, 67671: None, 67871: None, 67903: None, 68176: None, 68177: None, 68178: None, 68179: None, 68180: None, 68181: None, 68182: None, 68183: None, 68184: None, 68223: None, 68336: None, 68337: None, 68338: None, 68339: None, 68340: None, 68341: None, 68342: None, 68409: None, 68410: None, 68411: None, 68412: None, 68413: None, 68414: None, 68415: None, 68505: None, 68506: None, 68507: None, 68508: None, 69703: None, 69704: None, 69705: None, 69706: None, 69707: None, 69708: None, 69709: None, 69819: None, 69820: None, 69822: None, 69823: None, 69824: None, 69825: None, 69952: None, 69953: None, 69954: None, 69955: None, 70004: None, 70005: None, 70085: None, 70086: None, 70087: None, 70088: None, 70089: None, 70093: None, 70107: None, 70109: None, 70110: None, 70111: None, 70200: None, 70201: None, 70202: None, 70203: None, 70204: None, 70205: None, 70313: None, 70731: None, 70732: None, 70733: None, 70734: None, 70735: None, 70747: None, 70749: None, 70854: None, 71105: None, 71106: None, 71107: None, 71108: None, 71109: None, 71110: None, 71111: None, 71112: None, 71113: None, 71114: None, 71115: None, 71116: None, 71117: None, 71118: None, 71119: None, 71120: None, 71121: None, 71122: None, 71123: None, 71124: None, 71125: None, 71126: None, 71127: None, 71233: None, 71234: None, 71235: None, 71264: None, 71265: None, 71266: None, 71267: None, 71268: None, 71269: None, 71270: None, 71271: None, 71272: None, 71273: None, 71274: None, 71275: None, 71276: None, 71484: None, 71485: None, 71486: None, 72769: None, 72770: None, 72771: None, 72772: None, 72773: None, 72816: None, 72817: None, 74864: None, 74865: None, 74866: None, 74867: None, 74868: None, 92782: None, 92783: None, 92917: None, 92983: None, 92984: None, 92985: None, 92986: None, 92987: None, 92996: None, 113823: None, 121479: None, 121480: None, 121481: None, 121482: None, 121483: None, 125278: None, 125279: None}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft0bl62scTKQ",
        "colab_type": "text"
      },
      "source": [
        "## 6.4 Tokenizing Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYUGV-4bcTKR",
        "colab_type": "code",
        "colab": {},
        "outputId": "27cabe96-c4a0-420f-a145-5171150b25d3"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "string = \"The science of today is the technology of tommorrow\"\n",
        "\n",
        "# tokenize words\n",
        "word_tokenize(string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tommorrow']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eete5YTIcTKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d213b1d0-5537-4608-9f8d-50d8d840c98a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "string = \"The science of today is the technology of tommorw. Tommorrow is today\"\n",
        "\n",
        "# tokenize sentences\n",
        "sent_tokenize(string)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The science of today is the technology of tommorw.', 'Tommorrow is today']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtRE_s16cTKg",
        "colab_type": "text"
      },
      "source": [
        "## 6.5 Removing Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q4g1yp0cTKo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b1168f7-8245-480d-c5b1-f7db1573f020"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "tokenized_words = ['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# remove stop words\n",
        "[word for word in tokenized_words if word not in stop_words]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['going', 'go', 'store', 'park']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SClP8tlycTKv",
        "colab_type": "text"
      },
      "source": [
        "## 6.6 Stemming Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUSERCLUcTKx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4dd9db55-a276-4ba1-e2e3-0f9d3d7fef2b"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
        "\n",
        "# create stemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# apply stemmer\n",
        "[porter.stem(word) for word in tokenized_words]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fmxTu11cTK4",
        "colab_type": "text"
      },
      "source": [
        "### See Also\n",
        "* Porter Stemming Algorithm (https://tartarus.org/martin/PorterStemmer/)\n",
        "\n",
        "## 6.7 Tagging Part of Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpR40GFEcTK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fce5d0c1-b379-48d8-87d7-d789dbd93d1d"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text_data = \"Chris loved outdoor running\"\n",
        "\n",
        "text_tagged = pos_tag(word_tokenize(text_data))\n",
        "\n",
        "text_tagged"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au2P6zbfcTLE",
        "colab_type": "text"
      },
      "source": [
        "NLTK uses the Penn Treebank parts for speech tags, some examples:\n",
        "\n",
        "| Tag | Parts of Speech |\n",
        "|---  |-----------------|\n",
        "|NNP| Proper noun, singular|\n",
        "|NN| Noun, singular or mass|\n",
        "|RB| Adverb|\n",
        "|VBD| Verb, past tense|\n",
        "|VBG| Verb, gerund or present participle|\n",
        "|JJ| Adjective|\n",
        "|PRP| Personal pronoun|\n",
        "\n",
        "Once the text has been tagged, we can use the tags to find certain parts of speech. For example, here are all nouns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBV5WaWBcTLG",
        "colab_type": "code",
        "colab": {},
        "outputId": "72655fe6-ea4a-4c02-e5eb-624bfa228e4b"
      },
      "source": [
        "[word for word, tag in text_tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chris']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsS42g6fcTLP",
        "colab_type": "code",
        "colab": {},
        "outputId": "2a2c196a-5f99-4771-f4b1-27b87d7a402a"
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "tweets = [\"I am eating a burrito for breakfast\",\n",
        "         \"Political science is an amazing field\",\n",
        "         \"San Francisco is an awesome city\"]\n",
        "\n",
        "tagged_tweets = []\n",
        "\n",
        "# tag each word and each tweet\n",
        "for tweet in tweets:\n",
        "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
        "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
        "\n",
        "# use one hot encoding to convert the tags into features\n",
        "one_hot_multi = MultiLabelBinarizer()\n",
        "one_hot_multi.fit_transform(tagged_tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ZHHPEPcTLc",
        "colab_type": "code",
        "colab": {},
        "outputId": "fb0e1bd1-2eff-4012-bf85-1e1604a511e5"
      },
      "source": [
        "# show feature names\n",
        "one_hot_multi.classes_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MEYDNh6cTLl",
        "colab_type": "code",
        "colab": {},
        "outputId": "aaeaa583-99a1-49dc-a42d-c0d43648b9a9"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import BigramTagger\n",
        "from nltk.tag import TrigramTagger\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "    \n",
        "# get some text from the Brown\n",
        "sentences = brown.tagged_sents(categories='news')\n",
        "\n",
        "# split into 4000 stences for training and 623 for testing\n",
        "train = sentences[:4000]\n",
        "test = sentences[4000:]\n",
        "\n",
        "# create backoff tagger\n",
        "unigram = UnigramTagger(train)\n",
        "bigram = BigramTagger(train, backoff=unigram)\n",
        "trigram = TrigramTagger(train, backoff=bigram)\n",
        "\n",
        "trigram.evaluate(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /Users/f00/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8174734002697437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj-JSOSjcTLs",
        "colab_type": "text"
      },
      "source": [
        "### See Also\n",
        "* https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "\n",
        "## 6.8 Encoding Text as a Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaEy8uMucTLu",
        "colab_type": "code",
        "colab": {},
        "outputId": "e85aeed6-6e6c-467f-d97e-5dc6fceb242c"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text_data = np.array(['I love Brazil. Brazil!', 'Sweden is best', 'Germany beats both'])\n",
        "\n",
        "count = CountVectorizer()\n",
        "bag_of_words = count.fit_transform(text_data)\n",
        "\n",
        "bag_of_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 8 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYnwA-3acTL7",
        "colab_type": "code",
        "colab": {},
        "outputId": "1f167ec1-b23c-4e57-8e9e-ed60695ed141"
      },
      "source": [
        "bag_of_words.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
              "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPEdpc9NcTL_",
        "colab_type": "code",
        "colab": {},
        "outputId": "4bfbd91d-9772-4336-d81f-65c54d2d23c9"
      },
      "source": [
        "count.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaMovp3dcTMJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "cb62bdcc-8d9a-446d-e9d8-987b1c393ba6"
      },
      "source": [
        "count_2gram = CountVectorizer(ngram_range=(1,2), stop_words='english', vocabulary=['brazil'])\n",
        "bag = count_2gram.fit_transform(text_data)\n",
        "bag.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA4Ov6gOcTMP",
        "colab_type": "code",
        "colab": {},
        "outputId": "2651e4bf-013a-4c12-c0ce-bb5786257d1a"
      },
      "source": [
        "count_2gram.vocabulary_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'brazil': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzdC5MhCcTMV",
        "colab_type": "text"
      },
      "source": [
        "### See Also\n",
        "* n-gram (https://en.wikipedia.org/wiki/N-gram)\n",
        "* bag of words meets bags of popcorn (https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
        "\n",
        "## 6.9 Weighting Word Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA6oINyWcTMX",
        "colab_type": "code",
        "colab": {},
        "outputId": "e6fe4e5a-ea30-4297-d045-29c8db5fdca9"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "text_data = np.array(['I love Brazil. Brazil!', 'Sweden is best', 'Germany beats both'])\n",
        "\n",
        "# create the tf-idf feature matrix\n",
        "tfidf = TfidfVectorizer()\n",
        "feature_matrix = tfidf.fit_transform(text_data)\n",
        "\n",
        "feature_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 8 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC62qr2ucTMc",
        "colab_type": "code",
        "colab": {},
        "outputId": "072d56e1-acb9-4ce9-be83-51a4d71f1774"
      },
      "source": [
        "feature_matrix.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
              "        0.        , 0.4472136 , 0.        ],\n",
              "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
              "        0.57735027, 0.        , 0.57735027],\n",
              "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6tu6TXMcTMj",
        "colab_type": "code",
        "colab": {},
        "outputId": "a0569c87-be08-485e-91a5-3dd3d73f1aaa"
      },
      "source": [
        "tfidf.vocabulary_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'love': 6,\n",
              " 'brazil': 3,\n",
              " 'sweden': 7,\n",
              " 'is': 5,\n",
              " 'best': 1,\n",
              " 'germany': 4,\n",
              " 'beats': 0,\n",
              " 'both': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmUHubIMcTMp",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "tfidf(t, d) = tf(t,d) * idf(t)\n",
        "$$\n",
        "\n",
        "where $t$ is a word\n",
        "\n",
        "$d$ is a document\n",
        "\n",
        "$$\n",
        "idf(t) = log(\\frac{1 + n_d}{1 + df(d, t}) +1\n",
        "$$\n",
        "\n",
        "where $n_d$ is the number of documents and \n",
        "\n",
        "$df(d,t)$ is term, $t$'s document frequency (i.e. number of documents where the term appears)\n",
        "\n",
        "### See Also\n",
        "* scikit-learn documentation: tf-idf term weighting (http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sWe-7DwcTMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}